{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b9368c-a95a-4f03-8800-2577ac0b8b8b",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f906a57-1725-47be-85e5-4b2064ca5195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\subas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 18:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10260962694883347, 'eval_accuracy': 0.9636363636363636, 'eval_runtime': 19.6534, 'eval_samples_per_second': 2.799, 'eval_steps_per_second': 0.356, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and preprocess data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"MAIN_DATASET.csv\")\n",
    "texts = df[\"data\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "# Encode labels\n",
    "label2id = {label: idx for idx, label in enumerate(set(labels))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "encoded_labels = [label2id[label] for label in labels]\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, encoded_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Tokenize with RoBERTa\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Step 3: Create PyTorch Dataset\n",
    "import torch\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "\n",
    "# Step 4: Train the model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./results\")\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6494ce-052b-40c8-8243-f90348b99f16",
   "metadata": {},
   "source": [
    "### PDF VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad29f869-a72b-41e9-b367-9bd915941a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Checking File: NIO Dissertation.pdf\n",
      "\n",
      " Page 1: NIO Dissertation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicted: Request Letter\n",
      "\n",
      " Page 2: NIO Dissertation.pdf\n",
      " Predicted: Consent Email\n",
      "\n",
      " Page 3: NIO Dissertation.pdf\n",
      " Predicted: Application Form\n",
      "\n",
      " Page 4: NIO Dissertation.pdf\n",
      " Predicted: Resume\n",
      "\n",
      " Page 5: NIO Dissertation.pdf\n",
      " Duplicate: Resume\n",
      "\n",
      " Page 6: NIO Dissertation.pdf\n",
      " Predicted: ID Card\n",
      "\n",
      " Saved results to: application_validation_results_classified.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF File</th>\n",
       "      <th>Sections Found</th>\n",
       "      <th>Missing Sections</th>\n",
       "      <th>Request Letter Missing Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NIO Dissertation.pdf</td>\n",
       "      <td>Application Form, Consent Email, ID Card, Requ...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PDF File                                     Sections Found  \\\n",
       "0  NIO Dissertation.pdf  Application Form, Consent Email, ID Card, Requ...   \n",
       "\n",
       "  Missing Sections Request Letter Missing Points  \n",
       "0                                           None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import easyocr\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer (required for prediction)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./results\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Expected key points in Request Letter\n",
    "request_letter_points = [\n",
    "    \"consent\", \"moral\", \"evaluated\", \"name of the supervising\",\n",
    "    \"confidentiality\", \"follow\", \"negligence\"\n",
    "]\n",
    "\n",
    "# Predict section using the model\n",
    "def predict_section(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1).item()\n",
    "    return pred\n",
    "\n",
    "# Perform OCR on a single image\n",
    "def perform_ocr(image):\n",
    "    img_np = np.array(image)\n",
    "    results = reader.readtext(img_np)\n",
    "    return \"\\n\".join([text for _, text, _ in results]).lower()\n",
    "\n",
    "# Process a single PDF\n",
    "def process_pdf(pdf_path, label_map):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    found_sections = set()\n",
    "    extra_info = {}\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        print(f\"\\n Page {i+1}: {os.path.basename(pdf_path)}\")\n",
    "        text = perform_ocr(image)\n",
    "\n",
    "        predicted_label_id = predict_section(text)\n",
    "        predicted_section = label_map.get(predicted_label_id, None)\n",
    "\n",
    "        if predicted_section:\n",
    "            if predicted_section not in found_sections:\n",
    "                found_sections.add(predicted_section)\n",
    "                print(f\" Predicted: {predicted_section}\")\n",
    "            else:\n",
    "                print(f\" Duplicate: {predicted_section}\")\n",
    "\n",
    "            if predicted_section == \"Request Letter\":\n",
    "                missing = [pt for pt in request_letter_points if pt not in text]\n",
    "                extra_info['RequestLetterMissingPoints'] = \", \".join(missing) if missing else \"None\"\n",
    "        else:\n",
    "            print(\" Predicted section not in known label_map (ignored).\")\n",
    "\n",
    "    return found_sections, extra_info\n",
    "\n",
    "# Validate all PDFs in a folder\n",
    "\n",
    "def validate_all_pdfs(folder_path, label_map):\n",
    "    results = []\n",
    "    known_labels = set(label_map.values())\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(f\"\\n=> Checking File: {filename}\")\n",
    "            found_sections, extra_info = process_pdf(pdf_path, label_map)\n",
    "\n",
    "            # Only count known sections as missing\n",
    "            missing = known_labels - found_sections\n",
    "\n",
    "            results.append({\n",
    "                \"PDF File\": filename,\n",
    "                \"Sections Found\": \", \".join(sorted(found_sections)),\n",
    "                \"Missing Sections\": \", \".join(sorted(missing)) if missing else \"\",\n",
    "                \"Request Letter Missing Points\": extra_info.get('RequestLetterMissingPoints', '')\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"application_validation_results_classified.csv\", index=False)\n",
    "    print(\"\\n Saved results to: application_validation_results_classified.csv\")\n",
    "    return df\n",
    "\n",
    "# Run validation\n",
    "label_map = {v: k for k, v in label2id.items()}\n",
    "df_results = validate_all_pdfs(\"input\", label_map)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3a22d-31bb-4171-b387-00ac8836b17f",
   "metadata": {},
   "source": [
    "### ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f85e0a-2186-476e-9f53-c84d68e7d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACCURACY: 0.9927007299270073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict using your model and tokenizer\n",
    "def predict_label(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pred_id = torch.argmax(probs, dim=-1).item()\n",
    "    return label_map.get(pred_id, \"Unknown\")\n",
    "\n",
    "df[\"predicted_label\"] = df[\"data\"].apply(predict_label)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(df[\"label\"], df[\"predicted_label\"])\n",
    "print(\" ACCURACY:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa86e9-0e9b-4134-b389-aaa549a0a84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
